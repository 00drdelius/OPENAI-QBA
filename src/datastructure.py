from dataclasses import dataclass
from functools import partial
import tiktoken
from typing import Optional,List,Iterator

@dataclass
class CompletionUsage:
    completion_tokens: int
    """Number of tokens in the generated completion."""
    prompt_tokens: int
    """Number of tokens in the prompt."""
    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""


# Normal ChatCompletion Data Structure
@dataclass
class ChatCompletionMsg:
    content: Optional[str]
    "The contnets of the message."

@dataclass
class Choice:
    message: ChatCompletionMsg
    "A chat completion message generated by the model."

@dataclass
class ChatCompletion:
    choices: List[Choice]
    "A list of chat completion choices"
    model:str
    "The model used for the chat completion."
    usage:Optional[CompletionUsage]=None
    """Usage statistics for the completion request."""


# Streaming ChatCompletion Data Structure
@dataclass
class ChoiceDelta:
    content: Optional[str]=None
    "The content of the chunk message"

@dataclass
class ChoiceChunk:
    delta: ChoiceDelta
    "A chat completion delta generated by streamed model response."

@dataclass
class ChatCompletionChunk:
    choices:List[ChoiceChunk]
    "A list of chat completion chunk choices"
    model: str
    "The model used for the chat completion"

class Stream:
    def __init__(self,response:str,model:str) -> None:
        self.response=response
        self.encoder=tiktoken.get_encoding("cl100k_base")
        self.freeze_chunk=partial(ChatCompletionChunk,model=model)
        self._iterator=self.__stream__()

    def __next__(self):
        return self._iterator.__next__()
    
    def __iter__(self)->Iterator[ChatCompletionChunk]:
        for i in self._iterator:
            yield i

    def __stream__(self)->Iterator[ChatCompletionChunk]:
        encoding=self.encoder.encode(self.response)
        for token in encoding:
            delta=ChoiceDelta(content=self.encoder.decode_single_token_bytes(token).decode(encoding='utf8'))
            choices=[ChoiceChunk(delta=delta)]
            chunk=self.freeze_chunk(choices=choices)
            yield chunk

        # Ensure the entire stream is Done.
        yield self.freeze_chunk(choices=[ChoiceChunk(delta=ChoiceDelta(content=None))])

def construct_gptDS(stream:bool,response:str,model:str)-> ChatCompletion | Stream[ChatCompletionChunk]:
    if stream:
        return Stream(response,model)
    else:
        usage=CompletionUsage(0,0,0)
        msg=ChatCompletionMsg(content=response)
        choices=[Choice(msg)]
        return ChatCompletion(choices,model,usage)

if __name__=="__main__":
    import time
    from rich import print
    response="I'm Delius and I highly recommend you genshin impact this game."
    stream=Stream(response,"gpt-4-0125-preview")
    for chunk in stream:
        content=chunk.choices[0].delta.content
        if content==None:
            print("\n**encounter None!**\n")
            break
        print(content,end="",flush=False)
        time.sleep(0.2)
